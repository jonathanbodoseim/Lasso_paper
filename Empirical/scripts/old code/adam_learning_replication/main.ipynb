{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e06d475e",
   "metadata": {},
   "source": [
    "STOCK MARKET VOLATILITY AND LEARNING\n",
    "Adam, Marcet, Nicolini (2006-)\n",
    "\n",
    "MAIN PROGRAM to:\n",
    "- Load the data and compute sample moments (calls computeWeightMatrix)\n",
    "- Set the options (model, estimate or not, which statistics to match, weighting matrix)\n",
    "- Simulate (or estimate) Learning model (calls est_learning)\n",
    "- Simulate (or estimate) Abel RA model (calls est_Abel) - can be ignored\n",
    "- Show tables on the screen (calls show_table)\n",
    "\n",
    "Code originally by Davide Debortoli, September 2007\n",
    "Clearing and Comment by Tongbin Zhang, Feb, 2015\n",
    "Python translation, 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db318afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from scipy.optimize import minimize, fmin\n",
    "from scipy.linalg import inv, eig\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Optional, Dict, List\n",
    "import warnings\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7a101e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Configuration Classes\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration for the learning model\"\"\"\n",
    "    realizations: int = 1000\n",
    "    CI: float = 0.95\n",
    "    init: int = 2\n",
    "    lags: int = 5\n",
    "    PD_max: float = 500\n",
    "    ratio_proj: float = 5/2.5\n",
    "    method: str = 'tracking'  # 'OLS', 'tracking', or 'switching_weights'\n",
    "    proj_type: int = 1\n",
    "    C_DIV: int = 1  # 0 for baseline, 1 for C‚â†D\n",
    "    \n",
    "@dataclass\n",
    "class Parameters:\n",
    "    \"\"\"Model parameters\"\"\"\n",
    "    a: float = 1.00016      # Average dividend growth\n",
    "    s: float = 0.02         # Std of delta ln(d)\n",
    "    sigma: float = 5        # Risk aversion\n",
    "    delta: float = 150      # Discount factor\n",
    "    alpha: float = 163.4043 # Learning parameter\n",
    "\n",
    "class GlobalData:\n",
    "    \"\"\"Container for global data\"\"\"\n",
    "    def __init__(self):\n",
    "        self.MOM_DATA = None\n",
    "        self.N = None\n",
    "        self.dS_Sw_dS = None\n",
    "        self.realizations = 1000\n",
    "        self.init = 2\n",
    "        self.CI = 0.95\n",
    "        self.ratio_proj = 5/2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e06f442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Data Loading and Preprocessing\n",
    "# ============================================================================\n",
    "\n",
    "def load_series_data(filepath: Optional[str] = None) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Load financial series data\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to data file (if None, generates synthetic data)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing PD, rs, rb, DivGrowth arrays\n",
    "    \"\"\"\n",
    "    if filepath:\n",
    "        # Load actual data from file\n",
    "        # This would be implemented based on your data format\n",
    "        data = pd.read_csv(filepath)\n",
    "        return {\n",
    "            'PD': data['PD'].values,\n",
    "            'rs': data['rs'].values,\n",
    "            'rb': data['rb'].values,\n",
    "            'DivGrowth': data['DivGrowth'].values\n",
    "        }\n",
    "    else:\n",
    "        # Generate synthetic data for demonstration\n",
    "        np.random.seed(123)\n",
    "        n_periods = 500\n",
    "        \n",
    "        PD = 100 + 20 * np.random.randn(n_periods).cumsum()\n",
    "        rs = 1.02 + 0.15 * np.random.randn(n_periods)\n",
    "        rb = 1.01 + 0.02 * np.random.randn(n_periods)\n",
    "        DivGrowth = 1.002 + 0.03 * np.random.randn(n_periods)\n",
    "        \n",
    "        return {'PD': PD, 'rs': rs, 'rb': rb, 'DivGrowth': DivGrowth}\n",
    "\n",
    "def compute_rho(data: Dict[str, np.ndarray]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute autocorrelation coefficients\n",
    "    Equivalent to checkRho.m\n",
    "    \"\"\"\n",
    "    PD = data['PD']\n",
    "    \n",
    "    # Create lagged product\n",
    "    PD_PDlag = np.concatenate([[np.nan], PD[1:] * PD[:-1]])\n",
    "    \n",
    "    # Remove NaN and last 20 observations\n",
    "    PD_PDlag = PD_PDlag[1:-20]\n",
    "    PDlag = PD[:-21]\n",
    "    PDlag2 = PDlag ** 2\n",
    "    PD_clean = PD[1:-20]\n",
    "    PD2 = PD_clean ** 2\n",
    "    \n",
    "    # Asymptotic approximation\n",
    "    rho_approx = (np.mean(PD_PDlag) - np.mean(PD_clean)**2) / \\\n",
    "                 (np.mean(PD2) - np.mean(PD_clean)**2)\n",
    "    \n",
    "    # Exact sample correlation\n",
    "    rho = (np.mean(PD_PDlag) - np.mean(PD_clean) * np.mean(PDlag)) / (\n",
    "        np.sqrt(np.mean(PD2) - np.mean(PD_clean)**2) * \n",
    "        np.sqrt(np.mean(PDlag2) - np.mean(PDlag)**2)\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'rho_approx': rho_approx,\n",
    "        'rho_exact': rho,\n",
    "        'corr': np.corrcoef(PD_clean, PDlag)[0, 1]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdef8b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Weight Matrix Computation\n",
    "# ============================================================================\n",
    "\n",
    "def compute_weight_matrix(lag: int = 5, method: int = 1, \n",
    "                         do_fullsample: int = 0) -> Tuple[np.ndarray, np.ndarray, int]:\n",
    "    \"\"\"\n",
    "    Compute moments and weighting matrix\n",
    "    Equivalent to computeWeightMatrix.m\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    data = load_series_data()\n",
    "    PD = data['PD']\n",
    "    rs = data['rs']\n",
    "    rb = data['rb']\n",
    "    DivGrowth = data['DivGrowth']\n",
    "    \n",
    "    # Build additional series\n",
    "    PD_PDlag = np.concatenate([[np.nan], PD[1:] * PD[:-1]])\n",
    "    rs_rslag = np.concatenate([[np.nan], rs[1:] * rs[:-1]])\n",
    "    \n",
    "    # Create indices\n",
    "    stock_index = np.cumprod(rs[1:])\n",
    "    bond_index = np.cumprod(rb[1:])\n",
    "    \n",
    "    # Compute excess returns\n",
    "    if len(stock_index) > 4*lag:\n",
    "        excret = (stock_index[4*lag:] / stock_index[:-4*lag]) / \\\n",
    "                 (bond_index[4*lag:] / bond_index[:-4*lag])\n",
    "    else:\n",
    "        excret = np.array([])\n",
    "    \n",
    "    # Build h(y) vector - moments\n",
    "    end_idx = len(rs) - 4*lag\n",
    "    hy = np.array([\n",
    "        rs[1:end_idx],\n",
    "        rb[1:end_idx],\n",
    "        PD[1:end_idx],\n",
    "        DivGrowth[1:end_idx],\n",
    "        rs[1:end_idx]**2,\n",
    "        PD[1:end_idx]**2,\n",
    "        DivGrowth[1:end_idx]**2,\n",
    "        PD_PDlag[1:end_idx],\n",
    "        np.pad(excret, (0, max(0, end_idx-1-len(excret))), 'constant'),\n",
    "        np.pad(excret**2, (0, max(0, end_idx-1-len(excret))), 'constant'),\n",
    "        np.pad(excret * PD[:len(excret)], (0, max(0, end_idx-1-len(excret))), 'constant'),\n",
    "        rb[1:end_idx]**2,\n",
    "        rs_rslag[1:end_idx]\n",
    "    ])\n",
    "    \n",
    "    N = hy.shape[1]\n",
    "    M = np.mean(hy, axis=1)\n",
    "    \n",
    "    # Compute statistics vector S\n",
    "    S = compute_statistics_vector(M)\n",
    "    \n",
    "    # Compute derivatives\n",
    "    dS = compute_derivatives(S, M)\n",
    "    \n",
    "    # Compute Newey-West covariance\n",
    "    Sw = compute_newey_west(hy, M, N, method)\n",
    "    \n",
    "    # Weight matrix\n",
    "    dS_Sw_dS = dS @ Sw @ dS.T\n",
    "    \n",
    "    return dS_Sw_dS, S, N\n",
    "\n",
    "def compute_statistics_vector(M: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Compute statistics vector from moments\"\"\"\n",
    "    S = np.zeros(12)\n",
    "    S[0:4] = M[0:4]\n",
    "    S[4] = np.sqrt(max(0, M[4] - M[0]**2))     # sigma_rs\n",
    "    S[5] = np.sqrt(max(0, M[5] - M[2]**2))     # sigma_PD\n",
    "    S[6] = np.sqrt(max(0, M[6] - M[3]**2))     # sigma_D/d\n",
    "    S[7] = (M[7] - M[2]**2) / S[5]**2 if S[5] > 0 else 0  # autocorr_PD\n",
    "    S[8] = (M[10] - M[2]*M[8]) / S[5]**2 if S[5] > 0 else 0  # betaPD\n",
    "    S[9] = S[8]**2 * S[5]**2 / (M[9] - M[8]**2) if (M[9] - M[8]**2) > 0 else 0  # R^2\n",
    "    S[10] = np.sqrt(max(0, M[11] - M[1]**2))   # sigma_rb\n",
    "    S[11] = (M[12] - M[0]**2) / S[4]**2 if S[4] > 0 else 0  # autocorr_rs\n",
    "    return S\n",
    "\n",
    "def compute_derivatives(S: np.ndarray, M: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Compute derivatives dS/dM\"\"\"\n",
    "    dS = np.zeros((len(S), len(M)))\n",
    "    \n",
    "    # Identity mappings\n",
    "    for i in range(4):\n",
    "        dS[i, i] = 1\n",
    "    \n",
    "    # Standard deviations\n",
    "    if S[4] > 0:\n",
    "        dS[4, 4] = 1/(2*S[4])\n",
    "        dS[4, 0] = -M[0]/S[4]\n",
    "    if S[5] > 0:\n",
    "        dS[5, 5] = 1/(2*S[5])\n",
    "        dS[5, 2] = -M[2]/S[5]\n",
    "    if S[6] > 0:\n",
    "        dS[6, 6] = 1/(2*S[6])\n",
    "        dS[6, 3] = -M[3]/S[6]\n",
    "    if S[10] > 0:\n",
    "        dS[10, 11] = 1/(2*S[10])\n",
    "        dS[10, 1] = -M[1]/S[10]\n",
    "    \n",
    "    # Autocorrelations\n",
    "    if S[5] > 0:\n",
    "        dS[7, 7] = 1/S[5]**2\n",
    "        dS[7, 2] = 2*M[2]*(M[7]-M[5])/(S[5]**4)\n",
    "        dS[7, 5] = -2*(S[7]/S[5])*dS[5, 5]\n",
    "    \n",
    "    # Beta and R-squared\n",
    "    if S[5] > 0:\n",
    "        dS[8, 2] = (-M[8]*S[5]**2 - 2*S[5]*dS[5, 2]*(M[10]-M[2]*M[8]))/S[5]**4\n",
    "        dS[8, 5] = -2*(S[8]/S[5])*dS[5, 5]\n",
    "        dS[8, 8] = -M[2]/S[5]**2\n",
    "        dS[8, 10] = 1/S[5]**2\n",
    "    \n",
    "    return dS\n",
    "\n",
    "def compute_newey_west(hy: np.ndarray, M: np.ndarray, N: int, \n",
    "                       method: int = 1) -> np.ndarray:\n",
    "    \"\"\"Compute Newey-West covariance estimator\"\"\"\n",
    "    Mn = np.tile(M.reshape(-1, 1), (1, N))\n",
    "    hy_Mn = hy - Mn\n",
    "    \n",
    "    # Compute C0\n",
    "    C0 = (1/N) * (hy_Mn @ hy_Mn.T)\n",
    "    \n",
    "    if method == 1:  # Newey-West\n",
    "        K = int(N/4)\n",
    "        Sw = C0.copy()\n",
    "        \n",
    "        for j in range(1, K+1):\n",
    "            wjK = (1 - j/(K+1)) * (1/(N-j))\n",
    "            if j < hy_Mn.shape[1]:\n",
    "                Cj = wjK * hy_Mn[:, j:] @ hy_Mn[:, :-j].T\n",
    "                Sw = Sw + (Cj + Cj.T)\n",
    "    else:\n",
    "        # Den Haan Levin method (not implemented)\n",
    "        Sw = C0\n",
    "    \n",
    "    return Sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4ccf093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Learning Model Implementation\n",
    "# ============================================================================\n",
    "\n",
    "def est_learning_twoshocks(param_est: np.ndarray, param_fix: np.ndarray, \n",
    "                           PD_max: float, method: str = 'tracking',\n",
    "                           proj_type: int = 1, lags: int = 5, \n",
    "                           VCV: Optional[np.ndarray] = None,\n",
    "                           stat_sel: Optional[np.ndarray] = None,\n",
    "                           par_select: Optional[np.ndarray] = None,\n",
    "                           ModelWeightM: int = 0, onlydiag: int = 0,\n",
    "                           g: Optional[GlobalData] = None) -> Tuple:\n",
    "    \"\"\"\n",
    "    Simulate learning model with two shocks\n",
    "    \"\"\"\n",
    "    if g is None:\n",
    "        g = GlobalData()\n",
    "    \n",
    "    # Combine parameters\n",
    "    param = np.zeros(5)\n",
    "    if par_select is not None:\n",
    "        param[par_select] = param_est\n",
    "        param[~par_select] = param_fix\n",
    "    else:\n",
    "        param = param_est\n",
    "    \n",
    "    # Transform parameters back\n",
    "    param = np.exp(param)\n",
    "    param[0] = param[0] + 1\n",
    "    param[3] = param[3] / (1 + param[3])\n",
    "    param[[1, 2, 4]] = param[[1, 2, 4]] + 1e-3\n",
    "    \n",
    "    # Extract parameters\n",
    "    a, s, sigma, delta, alpha_init = param\n",
    "    \n",
    "    # Consumption shock parameters\n",
    "    s_c = s / 7\n",
    "    corr_cd = 0.2\n",
    "    cov_cd = corr_cd * s_c * s\n",
    "    SIGMA = np.array([[s**2, cov_cd], [cov_cd, s_c**2]])\n",
    "    \n",
    "    # Risk-adjusted price growth\n",
    "    bRE = a**(1-sigma) * np.exp(sigma*(1+sigma)*s_c**2/2 - sigma*cov_cd)\n",
    "    \n",
    "    # Check validity\n",
    "    if delta*bRE > PD_max/(PD_max + g.ratio_proj) or bRE < 1e-3:\n",
    "        return 1e10, None, None, None, None, None, param\n",
    "    \n",
    "    # Draw shocks\n",
    "    np.random.seed(123)\n",
    "    n_shocks = g.realizations * (g.N + g.init)\n",
    "    shocks = np.random.multivariate_normal([0, 0], SIGMA, n_shocks)\n",
    "    \n",
    "    # Reshape shocks\n",
    "    Shocks_Div = np.exp(shocks[:, 0].reshape(g.realizations, g.N + g.init) - s**2/2)\n",
    "    Shocks_Con = np.exp(shocks[:, 1].reshape(g.realizations, g.N + g.init) - s_c**2/2)\n",
    "    \n",
    "    # Initialize arrays\n",
    "    b = np.zeros((g.realizations, g.N))\n",
    "    alpha = np.ones(g.realizations) * alpha_init\n",
    "    b[:, 0] = bRE\n",
    "    PD = np.zeros((g.realizations, g.N))\n",
    "    PD[:, 0] = delta * bRE / (1 - delta * b[:, 0])\n",
    "    b_realiz = np.ones(g.realizations) * bRE\n",
    "    rb = np.zeros((g.realizations, g.N))\n",
    "    \n",
    "    # Simulation loop\n",
    "    for t in range(1, g.N + g.init):\n",
    "        if t < g.N:\n",
    "            # Update learning parameter\n",
    "            if method.lower() == 'ols':\n",
    "                alpha = alpha + 1\n",
    "            elif method.lower() == 'tracking':\n",
    "                alpha = np.ones(g.realizations) * alpha_init\n",
    "            elif method.lower() == 'switching_weights':\n",
    "                nu = 0.1\n",
    "                do_switch = np.abs(b_realiz - b[:, t-1]) <= nu\n",
    "                alpha[do_switch] = alpha[do_switch] + 1\n",
    "                alpha[~do_switch] = alpha_init\n",
    "            \n",
    "            # Belief updating\n",
    "            b[:, t] = b[:, t-1] + (1/alpha) * (b_realiz - b[:, t-1])\n",
    "            \n",
    "            # Projection facility\n",
    "            epsilon = bRE / PD_max\n",
    "            if proj_type == 1:\n",
    "                bpos = b[:, t] > (1/delta - g.ratio_proj * epsilon)\n",
    "                if np.any(bpos):\n",
    "                    weight = 1 - epsilon / (b[bpos, t] - 1/delta + (g.ratio_proj + 1) * epsilon)\n",
    "                    b[bpos, t] = weight * (1/delta - epsilon) + (1 - weight) * (1/delta - g.ratio_proj * epsilon)\n",
    "            \n",
    "            # Update prices and returns\n",
    "            PD[:, t] = delta * bRE / (1 - delta * b[:, t])\n",
    "            rb[:, t] = (delta * a**(-sigma) * np.exp(sigma*(1+sigma)*s_c**2/2))**(-1)\n",
    "            b_realiz = ((a * Shocks_Div[:, t]) * (a * Shocks_Con[:, t])**(-sigma) * \n",
    "                       (1 - delta * b[:, t-1]) / (1 - delta * b[:, t]))\n",
    "    \n",
    "    # Compute statistics\n",
    "    stats_dict = compute_model_statistics(PD, rb, Shocks_Div, Shocks_Con, a, lags, g)\n",
    "    \n",
    "    # Compute objective\n",
    "    obj = 0\n",
    "    if VCV is not None and stat_sel is not None and g.MOM_DATA is not None:\n",
    "        WeightM = inv(VCV[np.ix_(stat_sel, stat_sel)])\n",
    "        if onlydiag:\n",
    "            WeightM = inv(np.diag(np.diag(VCV[np.ix_(stat_sel, stat_sel)])))\n",
    "        \n",
    "        diff = g.MOM_DATA[stat_sel] - stats_dict['mean'][stat_sel]\n",
    "        obj = diff.T @ WeightM @ diff\n",
    "        \n",
    "        if not ModelWeightM:\n",
    "            T = g.N - 4*lags - 1\n",
    "            obj = T * obj\n",
    "    \n",
    "    return (obj, stats_dict.get('vcv'), stats_dict.get('mean'),\n",
    "            stats_dict.get('ci'), stats_dict.get('std'),\n",
    "            stats_dict.get('add_stats'), param)\n",
    "\n",
    "def compute_model_statistics(PD: np.ndarray, rb: np.ndarray, \n",
    "                            Shocks_Div: np.ndarray, Shocks_Con: np.ndarray,\n",
    "                            a: float, lags: int, g: GlobalData) -> Dict:\n",
    "    \"\"\"Compute model statistics from simulation\"\"\"\n",
    "    init = g.init\n",
    "    \n",
    "    # Stock returns - ensure compatible shapes\n",
    "    # PD has shape (realizations, N), Shocks_Div has shape (realizations, N+init)\n",
    "    n_periods = PD.shape[1] - 1\n",
    "    rs = ((1 + PD[:, 1:]) / PD[:, :-1]) * (a * Shocks_Div[:, init+1:init+1+n_periods])\n",
    "    \n",
    "    # Create indices\n",
    "    stock_index = np.cumprod(rs, axis=1)\n",
    "    bond_index = np.cumprod(rb[:, 1:], axis=1)\n",
    "    \n",
    "    # Adjust for lags\n",
    "    if rs.shape[1] <= 4*lags:\n",
    "        # Not enough data for statistics\n",
    "        return {\n",
    "            'mean': np.zeros(12),\n",
    "            'std': np.zeros(12),\n",
    "            'ci': (np.zeros(12), np.zeros(12)),\n",
    "            'vcv': np.eye(12),\n",
    "            'add_stats': np.zeros(3)\n",
    "        }\n",
    "    \n",
    "    # Means - adjust indices properly\n",
    "    Mean_rs = np.mean(rs[:, :max(1, rs.shape[1]-4*lags)], axis=1)\n",
    "    Mean_rb = np.mean(rb[:, 1:max(2, rb.shape[1]-4*lags)], axis=1) if rb.shape[1] > 1 else np.zeros(g.realizations)\n",
    "    Mean_PD = np.mean(PD[:, 1:max(2, PD.shape[1]-4*lags)], axis=1) if PD.shape[1] > 1 else np.zeros(g.realizations)\n",
    "    Mean_D = np.mean(a * Shocks_Div[:, init+1:max(init+2, Shocks_Div.shape[1]-4*lags)], axis=1)\n",
    "    \n",
    "    # Standard deviations\n",
    "    sd_rs = np.std(rs[:, :max(1, rs.shape[1]-4*lags)], axis=1)\n",
    "    sd_PD = np.std(PD[:, 1:max(2, PD.shape[1]-4*lags)], axis=1) if PD.shape[1] > 1 else np.zeros(g.realizations)\n",
    "    sd_D = np.std(a * Shocks_Div[:, init+1:max(init+2, Shocks_Div.shape[1]-4*lags)], axis=1)\n",
    "    sd_rb = np.std(rb[:, 1:max(2, rb.shape[1]-4*lags)], axis=1) if rb.shape[1] > 1 else np.zeros(g.realizations)\n",
    "    \n",
    "    # Autocorrelations\n",
    "    autcorr_PD = compute_autocorr(PD[:, 1:max(2, PD.shape[1]-4*lags)])\n",
    "    autcorr_rs = compute_autocorr(rs[:, :max(1, rs.shape[1]-4*lags)])\n",
    "    \n",
    "    # Excess returns and regression\n",
    "    if stock_index.shape[1] > 4*lags and bond_index.shape[1] > 4*lags:\n",
    "        n_ret = min(stock_index.shape[1], bond_index.shape[1])\n",
    "        exc_ret = ((stock_index[:, 4*lags:n_ret] / stock_index[:, :n_ret-4*lags]) /\n",
    "                   (bond_index[:, 4*lags:n_ret] / bond_index[:, :n_ret-4*lags]))\n",
    "        betaPD, Rsquare = compute_regression_stats(PD[:, 1:exc_ret.shape[1]+1], exc_ret)\n",
    "    else:\n",
    "        betaPD = np.zeros(g.realizations)\n",
    "        Rsquare = np.zeros(g.realizations)\n",
    "    \n",
    "    # Combine statistics\n",
    "    Distr_STATS = np.column_stack([\n",
    "        Mean_rs, Mean_rb, Mean_PD, Mean_D,\n",
    "        sd_rs, sd_PD, sd_D, autcorr_PD,\n",
    "        betaPD, Rsquare, sd_rb, autcorr_rs\n",
    "    ])\n",
    "    \n",
    "    # Summary statistics\n",
    "    stats_mean = np.mean(Distr_STATS, axis=0)\n",
    "    stats_std = np.std(Distr_STATS, axis=0)\n",
    "    stats_sorted = np.sort(Distr_STATS, axis=0)\n",
    "    \n",
    "    CI_low = int((1 - g.CI) * g.realizations)\n",
    "    CI_high = int(g.CI * g.realizations)\n",
    "    \n",
    "    return {\n",
    "        'mean': stats_mean,\n",
    "        'std': stats_std,\n",
    "        'ci': (stats_sorted[CI_low, :], stats_sorted[CI_high, :]),\n",
    "        'vcv': np.cov(Distr_STATS.T),\n",
    "        'add_stats': np.array([\n",
    "            np.mean(np.log(rs[:, :-4*lags])),\n",
    "            np.mean(np.log(rb[:, init+1:-4*lags])),\n",
    "            np.mean(PD[:, -4*lags] / PD[:, 0])\n",
    "        ])\n",
    "    }\n",
    "\n",
    "def compute_autocorr(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Compute autocorrelation for each row\"\"\"\n",
    "    autocorr = np.zeros(x.shape[0])\n",
    "    for i in range(x.shape[0]):\n",
    "        if x.shape[1] > 1:\n",
    "            autocorr[i] = np.corrcoef(x[i, 1:], x[i, :-1])[0, 1]\n",
    "    return autocorr\n",
    "\n",
    "def compute_regression_stats(PD: np.ndarray, exc_ret: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Compute regression statistics\"\"\"\n",
    "    betaPD = np.zeros(PD.shape[0])\n",
    "    Rsquare = np.zeros(PD.shape[0])\n",
    "    \n",
    "    for i in range(PD.shape[0]):\n",
    "        if exc_ret.shape[1] > 0:\n",
    "            n = min(PD.shape[1], exc_ret.shape[1])\n",
    "            if n > 1:\n",
    "                corr = np.corrcoef(PD[i, :n], exc_ret[i, :n])[0, 1]\n",
    "                Rsquare[i] = corr**2\n",
    "                betaPD[i] = corr * (np.std(exc_ret[i, :n]) / np.std(PD[i, :n]))\n",
    "    \n",
    "    return betaPD, Rsquare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74eeb2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# R-squared Computation\n",
    "# ============================================================================\n",
    "\n",
    "def compute_rsquare(dS_Sw_dS: np.ndarray) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute R-squared statistics\n",
    "    \"\"\"\n",
    "    # Take first 10x10 submatrix\n",
    "    Sigma = dS_Sw_dS[:10, :10]\n",
    "    n = len(Sigma)\n",
    "    \n",
    "    Rsquared = np.zeros(n)\n",
    "    \n",
    "    for i in range(n):\n",
    "        mask = np.ones(n, dtype=bool)\n",
    "        mask[i] = False\n",
    "        \n",
    "        V_i = Sigma[np.ix_(mask, mask)]\n",
    "        covi_i = Sigma[mask, i]\n",
    "        Vi = Sigma[i, i]\n",
    "        \n",
    "        if Vi > 0 and np.linalg.det(V_i) != 0:\n",
    "            Rsquared[i] = 1 - (covi_i.T @ inv(V_i) @ covi_i) / Vi\n",
    "    \n",
    "    labels = [\n",
    "        'E(r^s)', 'E(r^b)', 'E(PD)', 'E(D/d)',\n",
    "        'sigma_r^s', 'sigma_PD', 'sigma_D/d',\n",
    "        'autcorr_PD', 'betaPD', 'R^2'\n",
    "    ]\n",
    "    \n",
    "    return pd.DataFrame({'Statistic': labels, 'R-squared': Rsquared})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "370f41f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Results Display\n",
    "# ============================================================================\n",
    "\n",
    "def show_results_table(STATS: np.ndarray, STD: np.ndarray, \n",
    "                      MOM_DATA: np.ndarray, std_MOM_DATA: np.ndarray,\n",
    "                      match_stat: np.ndarray) -> pd.DataFrame:\n",
    "    \"\"\"Display results table\"\"\"\n",
    "    vlab = [\n",
    "        'E(r^s)', 'E(r^b)', 'E(PD)', 'E(D/d)',\n",
    "        'sigma_r^s', 'sigma_PD', 'sigma_D/d', 'autcorr_PD',\n",
    "        'betaPD', 'R^2', 'sigma_r^b', 'autcorr_r^s'\n",
    "    ]\n",
    "    \n",
    "    # Scaling\n",
    "    scale_prod = np.array([100, 100, 1, 100, 100, 1, 100, 1, 1, 1, 100, 1])\n",
    "    scale_sum = np.array([-1, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "    \n",
    "    # Scale data\n",
    "    STATS_Table = (STATS + scale_sum) * scale_prod\n",
    "    MOM_DATA_Table = (MOM_DATA + scale_sum) * scale_prod\n",
    "    STD_Table = STD * scale_prod\n",
    "    std_MOM_DATA_Table = std_MOM_DATA * scale_prod\n",
    "    \n",
    "    # T-statistics\n",
    "    t_stats = (MOM_DATA_Table - STATS_Table) / std_MOM_DATA_Table\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'Statistic': vlab,\n",
    "        'Model Mean': STATS_Table,\n",
    "        'Model Std': STD_Table,\n",
    "        'Data Mean': MOM_DATA_Table,\n",
    "        'Data Std': std_MOM_DATA_Table,\n",
    "        't-stat': t_stats,\n",
    "        'Used': match_stat.astype(int)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a539dc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                      STOCK MARKET VOLATILITY AND LEARNING                      \n",
      "                  Adam, Marcet, Nicolini Model Implementation                   \n",
      "================================================================================\n",
      "\n",
      "üìä Computing weight matrix...\n",
      "üìà Computing R-squared statistics...\n",
      "    Statistic  R-squared\n",
      "0      E(r^s)        0.0\n",
      "1      E(r^b)        0.0\n",
      "2       E(PD)        0.0\n",
      "3      E(D/d)        0.0\n",
      "4   sigma_r^s        0.0\n",
      "5    sigma_PD        0.0\n",
      "6   sigma_D/d        0.0\n",
      "7  autcorr_PD        0.0\n",
      "8      betaPD        0.0\n",
      "9         R^2        0.0\n",
      "\n",
      "üîÑ Running simulation...\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLinAlgError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 92\u001b[39m\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     83\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mconfig\u001b[39m\u001b[33m'\u001b[39m: config,\n\u001b[32m     84\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m'\u001b[39m: param_res,\n\u001b[32m   (...)\u001b[39m\u001b[32m     88\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mobjective\u001b[39m\u001b[33m'\u001b[39m: obj\n\u001b[32m     89\u001b[39m     }\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     results = \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 55\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# Run simulation\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müîÑ Running simulation...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m obj, VCV_model, STATS, CINT, STD, ADD_STATS, param_res = \u001b[43mest_learning_twoshocks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparam_est\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_fix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPD_max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproj_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproj_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVCV\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdS_Sw_dS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstat_sel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstat_sel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpar_select\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpar_select\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mModelWeightM\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monlydiag\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m=\u001b[49m\u001b[43mg\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# Display results\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m STATS \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 104\u001b[39m, in \u001b[36mest_learning_twoshocks\u001b[39m\u001b[34m(param_est, param_fix, PD_max, method, proj_type, lags, VCV, stat_sel, par_select, ModelWeightM, onlydiag, g)\u001b[39m\n\u001b[32m    102\u001b[39m obj = \u001b[32m0\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m VCV \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m stat_sel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m g.MOM_DATA \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m     WeightM = \u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mVCV\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mix_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstat_sel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstat_sel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    105\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m onlydiag:\n\u001b[32m    106\u001b[39m         WeightM = inv(np.diag(np.diag(VCV[np.ix_(stat_sel, stat_sel)])))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonat\\anaconda3\\envs\\narratives_project\\Lib\\site-packages\\scipy\\_lib\\_util.py:1233\u001b[39m, in \u001b[36m_apply_over_batch.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1231\u001b[39m \u001b[38;5;66;03m# Early exit if call is not batched\u001b[39;00m\n\u001b[32m   1232\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(batch_shapes):\n\u001b[32m-> \u001b[39m\u001b[32m1233\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mother_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[38;5;66;03m# Determine broadcasted batch shape\u001b[39;00m\n\u001b[32m   1236\u001b[39m batch_shape = np.broadcast_shapes(*batch_shapes)  \u001b[38;5;66;03m# Gives OK error message\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jonat\\anaconda3\\envs\\narratives_project\\Lib\\site-packages\\scipy\\linalg\\_basic.py:1186\u001b[39m, in \u001b[36minv\u001b[39m\u001b[34m(a, overwrite_a, check_finite)\u001b[39m\n\u001b[32m   1184\u001b[39m     inv_a, info = getri(lu, piv, lwork=lwork, overwrite_lu=\u001b[32m1\u001b[39m)\n\u001b[32m   1185\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m info > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1186\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[33m\"\u001b[39m\u001b[33msingular matrix\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1187\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m info < \u001b[32m0\u001b[39m:\n\u001b[32m   1188\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1189\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33millegal value in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m-info\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-th argument of internal getrf|getri\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   1190\u001b[39m     )\n",
      "\u001b[31mLinAlgError\u001b[39m: singular matrix"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Main Execution\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\" STOCK MARKET VOLATILITY AND LEARNING \".center(80))\n",
    "    print(\" Adam, Marcet, Nicolini Model Implementation \".center(80))\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "    \n",
    "    # Initialize\n",
    "    g = GlobalData()\n",
    "    g.realizations = 1000\n",
    "    g.N = 500\n",
    "    \n",
    "    # Load data\n",
    "    print(\"üìä Computing weight matrix...\")\n",
    "    dS_Sw_dS, S, N = compute_weight_matrix(lag=5, method=1)\n",
    "    g.MOM_DATA = S\n",
    "    g.N = N\n",
    "    g.dS_Sw_dS = dS_Sw_dS\n",
    "    \n",
    "    # Compute R-squared\n",
    "    print(\"üìà Computing R-squared statistics...\")\n",
    "    rsquare_df = compute_rsquare(dS_Sw_dS)\n",
    "    print(rsquare_df)\n",
    "    print()\n",
    "    \n",
    "    # Model configuration\n",
    "    config = ModelConfig()\n",
    "    params = Parameters()\n",
    "    \n",
    "    # Setup parameters\n",
    "    param_guess = np.array([params.a, params.s, params.sigma, params.delta, params.alpha])\n",
    "    par_select = np.array([True, True, False, True, True])\n",
    "    match_stat = np.ones(len(S), dtype=bool)\n",
    "    stat_sel = np.arange(len(S))[match_stat]\n",
    "    \n",
    "    # Transform parameters\n",
    "    param = np.array([\n",
    "        np.log(params.a - 1),\n",
    "        np.log(params.s - 1e-3),\n",
    "        np.log(params.sigma - 1e-3),\n",
    "        np.log(params.delta - 1e-3),\n",
    "        np.log(params.alpha - 1e-3)\n",
    "    ])\n",
    "    \n",
    "    param_est = param[par_select]\n",
    "    param_fix = param[~par_select]\n",
    "    \n",
    "    # Run simulation\n",
    "    print(\"üîÑ Running simulation...\")\n",
    "    obj, VCV_model, STATS, CINT, STD, ADD_STATS, param_res = est_learning_twoshocks(\n",
    "        param_est, param_fix, config.PD_max,\n",
    "        method=config.method, proj_type=config.proj_type,\n",
    "        lags=config.lags, VCV=dS_Sw_dS, stat_sel=stat_sel,\n",
    "        par_select=par_select, ModelWeightM=0, onlydiag=0, g=g\n",
    "    )\n",
    "    \n",
    "    # Display results\n",
    "    if STATS is not None:\n",
    "        print(\"üìã RESULTS\")\n",
    "        BigSigma = dS_Sw_dS\n",
    "        std_MOM_DATA = np.diag(np.sqrt(BigSigma / (N - 4*config.lags - 1)))\n",
    "        \n",
    "        results_df = show_results_table(STATS, STD, g.MOM_DATA, std_MOM_DATA, match_stat)\n",
    "        print(results_df)\n",
    "        \n",
    "        # Test statistics\n",
    "        dgf = np.sum(match_stat) - np.sum(par_select)\n",
    "        if dgf > 0:\n",
    "            print(f\"\\nüîç Test Statistics:\")\n",
    "            print(f\"   Objective: {obj:.4f}\")\n",
    "            print(f\"   DOF: {dgf}\")\n",
    "            p_value = 1 - stats.chi2.cdf(obj, dgf)\n",
    "            print(f\"   p-value: {p_value:.4f}\")\n",
    "    \n",
    "    print(\"\\n‚úì Complete!\")\n",
    "    \n",
    "    return {\n",
    "        'config': config,\n",
    "        'params': param_res,\n",
    "        'statistics': STATS,\n",
    "        'confidence_intervals': CINT,\n",
    "        'std_dev': STD,\n",
    "        'objective': obj\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "narratives_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
